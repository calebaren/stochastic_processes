---
title: "Chapter 2"
author: "calebren"
date: "2025-10-02"
output: pdf_document
---
\newcommand{\Var}{\textrm{Var}}
\newcommand{\Cov}{\textrm{Cov}}
\newcommand{\soln}{\textbf{Solution:}}

```{r setup, include=FALSE}
knitr::opts_chunk$set(eval = FALSE, include = FALSE)
```

# Exercises
## Exercise 2.1
A Markov chain has transition matrix
$$
P = \bordermatrix{
  & 1 & 2 & 3 \cr
1 & 0.1 & 0.3 & 0.6 \cr
2 & 0   & 0.4 & 0.6 \cr
3 & 0.3 & 0.2 & 0.5 
}
$$
with initial distribution $\alpha = (0.2, 0.3, 0.5)$. Find the following:
(a) $P(X_7 = 3 | X_6 = 2)$

\soln By time homogeneity,
$$
P(X_7 = 3 | X_6 = 2) = P(X_1 = 3 | X_0 = 2) = P_{23} = 0.6
$$

(b) $P(X_9=2|X_1=2,X_5=1,X_7=3)$

\soln By the Markov property and time homogeneity, 
$$
\begin{gathered}
P(X_9=2|X_1=2,X_5=1,X_7=3) = P(X_9 =2| X_7 = 3) \\
= P(X_2=2|X_0=3) (P^2)_{23} \\
= 0.54
\end{gathered}
$$

(c) $P(X_0=3|X_1=1)$

\soln By Bayes' rule,
$$
\begin{gathered}
P(X_0 = 3|X_1=1) = \frac{P(X_1=1|X_0=3)P(X_0=3)}{P(X_1=1)} \cr
= \frac{P_{31}\alpha_3}{(\alpha P)_1} \cr
= \frac{0.3\cdot 0.5}{0.17} \cr
= \frac{15}{17} \approx 0.88
\end{gathered}
$$

```{r exercise 2.1c}
exercise_2_1c <- function() {
  # sample a bunch from alpha
  nsim <- 1000
  initial <- sample(1:3, size = nsim, replace = T, prob = alpha)
  
  # then, get the next state based on x
  nxt <- sapply(initial, FUN = function(i) sample(1:3, size = 1, prob = pmat[i,]))
  
  # calculate prob that initial = 3, based on next = 1.
  mean(initial[nxt == 1] == 3)
}
```

(d) $E(X_2)$

\soln 
$$
E(X_2) = \sum_k k P(X_2 = k) = \sum_k k (\alpha P^2)_k = 2.363
$$

```{r exercise 2.1d}
# start with initial distribution
exercise_2_1d <- function() {
  nsim <- 1e4
  states <- sample(1:3, size = nsim, replace = T, prob = alpha)
  states <- sapply(states, FUN = function(i) sample(1:3, size = 1, prob = pmat[i,]))
  states <- sapply(states, FUN = function(i) sample(1:3, size = 1, prob = pmat[i,]))
  mean(states)
}
```

## Exercise 2.2

Let $X_0, X_1, \dots$ be a Markov chain with transition matrix
$$
\bordermatrix{
& 1 & 2 & 3 \cr
1 & 0 & 1/2 & 1/2 \cr
2 & 1 & 0 & 0 \cr
3 & 1/3 & 1/3 & 1/3
}
$$
and initial distribution $\alpha = (1/2, 0, 1/2)$. Find the following:
(a) $P(X_2=1|X_1=3)$
\soln $P(X_2=1|X_1=3) = P_{31} = 1/3$

(b) $P(X_1=3, X_2=1)$
\soln $P(X_1=3,X_2=2) = (\alpha P)_3 P_{31}$

```{r}
initial <- sample(1:3, size = 1e4, replace = T, prob = alpha)
first <- sapply(initial, function(i) sample(1:3, size = 1, prob = pmat[i,]))
second <- sapply(first, function(i) sample(1:3, size = 1, prob = pmat[i,]))
mean(first == 3 & second == 1)
```

## Exercise 2.3 NOT STARTED

## Exercise 2.4
For the general two-state chain with transition matrix
$$
P = 
\bordermatrix{
& a & b \cr
a & 1-p & p \cr
b & q & 1-q
}
$$
and initial distribution $\alpha = (\alpha_1, \alpha_2)$, find the following:
(a) the two-step transition matrix
\soln

$$
\begin{aligned}
P^2 &= \begin{bmatrix}
1-p & p \\
q & 1-q
\end{bmatrix} \begin{bmatrix}
1-p & p \\
q & 1-q
\end{bmatrix} \\
&= \begin{bmatrix}
(1-p)^2 + pq & (1-p)p + p(1-q) \\
q(1-p) + q(1-q) & pq + (1-q)^2
\end{bmatrix} \\
&= \begin{bmatrix}
(1-p)^2 + pq & p(2-p-q) \\
q(2-p-q) & (1-q)^2 + pq
\end{bmatrix}
\end{aligned}
$$

(b) the distribution of $X_1$
\soln
$$
\begin{aligned}
X_1 &= \alpha P \\
&= \begin{bmatrix}\alpha_1 & \alpha_2\end{bmatrix} \begin{bmatrix}
1-p & p \\
q & 1-q
\end{bmatrix} \\
&= \begin{bmatrix}
\alpha_1-\alpha_1p + \alpha_2q & \alpha_1p + \alpha_2-\alpha_2 q
\end{bmatrix}
\end{aligned}
$$

## Exercise 2.5
Consider a random walk on $\{0,\dots,k\}$, which moves left and right with respective probabilities $q$ and $p$. If the walk is at 0 it transitions to 1 on the next step. If the walk is at $k$ it transitions to $k-1$ on the next step. This is called \textrm{random walk with reflecting boundaries}. Assume that $k=3, q=1/4, p=3/4$, and the initial distribution is uniform. For the following, use technology if needed.

(a) Exhibit the transition matrix.

\soln The transition matrix is:
$$
P = \bordermatrix{
  & 0 & 1 & 2 & 3 \cr
0 & 0 & 1 & 0 & 0 \cr
1 & q & 0 & p & 0 \cr
2 & 0 & q & 0 & p \cr
3 & 0 & 0 & 1 & 0
} \\
= \bordermatrix{
  & 0 & 1 & 2 & 3 \cr
0 & 0 & 1 & 0 & 0 \cr
1 & 1/4 & 0 & 3/4 & 0 \cr
2 & 0 & 1/4 & 0 & 3/4 \cr
3 & 0 & 0 & 1 & 0
}
$$

(b) Find $P(X_7=1|X_0=3,X_2=2,X_4=2)$.

\soln By the Markov property, this probability depends on the most recent state, so $P = P(X_7 = 1 | X_4 = 2) = (P^3)_{21} = 19/64$.

$$
\begin{aligned}
P^3 &= \begin{bmatrix}
 0 & 1 & 0 & 0 \cr
 1/4 & 0 & 3/4 & 0 \cr
0 & 1/4 & 0 & 3/4 \cr
0 & 0 & 1 & 0
\end{bmatrix} \begin{bmatrix}
 0 & 1 & 0 & 0 \cr
 1/4 & 0 & 3/4 & 0 \cr
0 & 1/4 & 0 & 3/4 \cr
0 & 0 & 1 & 0
\end{bmatrix} \begin{bmatrix}
 0 & 1 & 0 & 0 \cr
 1/4 & 0 & 3/4 & 0 \cr
0 & 1/4 & 0 & 3/4 \cr
0 & 0 & 1 & 0
\end{bmatrix} \\
&= \begin{bmatrix}
 1/4 & 0 & 3/4 & 0 \cr
0 & 7/16 & 0 & 9/16 \cr
1/16 & 0 & 15/16 & 0 \cr
0 & 1/4 & 0 & 3/4 
\end{bmatrix} \begin{bmatrix}
 0 & 1 & 0 & 0 \cr
 1/4 & 0 & 3/4 & 0 \cr
0 & 1/4 & 0 & 3/4 \cr
0 & 0 & 1 & 0
\end{bmatrix} \\
&= \begin{bmatrix}
0 & 7/16 & 0 & 9/16 \cr
7/64 & 0 & 57/64 & 0 \cr
0 & 19/64 & 0 & 45/64 \cr
1/16 & 0 & 15/16 & 0
\end{bmatrix}
\end{aligned}
$$

(c) Find $P(X_3=1, X_5=3)$.

\soln $P(X_3=1,X_5=3) = (\alpha P^3)_1 (P^2)_{13} = 0.103$

```{r}
exercise_2_5 <- function() {
  steps <- 5
  nsim <- 1e5
  
  data <- matrix(nrow = nsim, ncol = steps+1)
  pmat <- matrix(c(0, 1, 0, 0, 1/4, 0, 3/4, 0, 0, 1/4, 0, 3/4, 0, 0, 1, 0), nrow = 4, byrow = T)
  
  # initial state
  data[,1] <- sample(1:4, size = nsim, replace = T)
  for (j in 2:(steps+1)) {
    for (k in 1:nsim) {
      data[k, j] <- sample(1:4, size = 1, prob = pmat[data[k,(j-1)],])
    }
  }
  data[,4]==2 & data[,6]==4
}
```

## Exercise 2.6

A tetrahedron die has four faces labeled 1,2,3, and 4. In repeated independent rolls of the die $R_0, R_1, \dots,$ let $X_n = \max\{R_0,\dots,R_n\}$ be the maximum value after $n+1$ rolls, for $\ge 0$.
(a) Give an intuitive argument for why $X_0, X_1,\dots,$ is a Markov chain, and exhibit the transition matrix.

\soln Each subsequent roll is independent from the previous rolls. In order to determine what $X_n$ is, we only need the state of $X_{n-1}$ and the current roll, which is independent of any prior states or future states. The transition matrix is:
$$
P = \bordermatrix{
  & 1 & 2 & 3 & 4 \cr
1 & 1/4 & 1/4 & 1/4 & 1/4 \cr
2 & 0  & 1/2 & 1/4 & 1/4 \cr
3 & 0 & 0 & 3/4 & 1/4 \cr
4 & 0 & 0 & 0 & 1
}
$$

(b) Find $P(X_3 \ge 3)$.

\soln
$$
P(X_3 \ge 3) = P(X_3 = 3) + P(X_3=4) = 0.9375
$$

```{r}
exercise_2_6 <- function() {
  pmat <- matrix(c(0.25, 0.25, 0.25, 0.25,
                   0, 0.5, 0.25, 0.25,
                   0, 0, 0.75, 0.25,
                   0, 0, 0, 1),
                 byrow = T, nrow = 4)
  rep(0.25, 4) %*% (pmat %^% 3)
}

exercise_2_6()
```

## Exercise 2.7
Let $X_0, X_1, \dots$ be a Markov chain with transition matrix $P$. Let $Y_n = X_{3n}$, for $n=0, 1, 2, \dots$. Show that $Y_0, Y_1,\dots$ is a Markov chain and exhibit its transition matrix.

\soln Since $X_0, X_1, \dots$ is a Markov chain, then $P(X_{3n} = j | X_{3n-3} = i)$ is Markov by the Markov property. Therefore, $P(Y_n = j | Y_{n-1} = i)$ is Markov, with transition matrix $P^3$.

## Exercise 2.8
Give the Markov transition matrix for random walk on the weighted graph in Figure 2.10.

\soln
$$
P = \bordermatrix{
  & a & b & c & d & e \cr
a & 0 & 1/6 & 1/2 & 0 & 1/3 \cr
b & 1/10 & 1/5 & 1/5 & 1/10 & 2/5 \cr
c & 1/2 & 1/3 & 0 & 1/6 & 0 \cr
d & 0 & 1/2 & 1/2 & 0 & 0 \cr
e & 1/3 & 2/3 & 0 & 0 & 0
}
$$

## Exercise 2.9
Give the transition matrix for the transition graph in Figure 2.11.

\soln
$$
P = \bordermatrix{
& a & b & c & d & e \cr
a & 0 & 0 & 3/5 & 0 & 2/5 \cr
b & 1/7 & 2/7 & 0 & 0 & 4/7 \cr
c & 0 & 2/9 & 2/3 & 1/9 & 0 \cr
d & 0 & 1 & 0 & 0 & 0 \cr
e & 1 & 0 & 0 & 0 & 0
}
$$

## Exercise 2.10 NOT STARTED

## Exercise 2.11
You start with five dice. Roll all the dice and put aside those dice that come up 6. Then, roll the remaining dice, putting aside those dice that come up 6. And so on. Let $X_n$ be the number of dice that are sixes are $n$ rolls.
(a) Describe the transition matrix $P$ for this Markov chain.

\soln The number of dice that are 6 is increasing for each roll. There are 0 dice that are 6s initially, and the player has 5 independent dice roll with probability 1/6 of getting a 6. In fact, if the player has seen $X_n$ face up dice, then there are $5-X_n$ trials to transition.
$$
X_n | X_{n-1} = X_{n-1} + D_n
$$
where
$$
D_n =  \textrm{Binom}(5-X_{n-1}, 1/6)
$$

(b) Find the probability of getting all sixes by the third play.

\soln This probability is $P(X_3 = 5) = (\alpha P)_5 = [(1, 0, 0, 0, 0, 0)^T P]_5 = 0.013$

```{r}
exercise_2_11b <- function() {
  P <- matrix(0, nrow = 6, ncol = 6)

  for (i in 0:5) {
    for (j in i:5) {
      P[i+1, j+1] <- dbinom(j - i, size = 5 - i, prob = 1/6)
    }
  }
  c(1, 0, 0, 0, 0, 0) %*% (P %^% 3)
}

exercise_2_11()
```

(c) What do you expect $P^100$ to look like? Use technology to confirm your answer.

\soln As the number of rolls increases, the chance of landing a 6 for each dice almost surely converges to 1, so $P^100$ should look very much like every state transitions to 5 regardless of initial state.

```{r echo=TRUE}
exercise_2_11c <- function(n) {
  P <- matrix(0, nrow = 6, ncol = 6)

  for (i in 0:5) {
    for (j in i:5) {
      P[i+1, j+1] <- dbinom(j - i, size = 5 - i, prob = 1/6)
    }
  }
   
  P %^% 100
}
exercise_2_11c()
```

## Exercise 2.12
Two urns contain $k$ balls each. Initially, the balls in the left urn are all red and the balls in the right urn are all blue. At each step, pick a ball at random from each urn and exchange them. Let $X_n$ be the number of balls in the left urn. (Note that necessarily $X_0 = 0$ and $X_1=1$.) Argue that the process is a Markov chain. Find the transition matrix. This model is called the Bernoulli-Laplace model of diffusion and was introduced by Daniel Bernoulli in 1769 as a model for the flow of two incompressible liquids between two containers.

\soln This is a Markov chain because the probability of transitioning from one state to the next is dependent only on the current balance of red/blue balls between the two urns, not on the states that came previously. If there are $i < k$ blue balls in the left urn, then these things can happen: 1) If we draw 1 blue ball from the left urn and 1 red ball from the right, then we transition to $i-1$ blue balls. 2) If we draw 1 red ball from the left urn and 1 blue ball from the right, then we transition to $i+1$ blue balls. 3) If we draw 2 blue balls or 2 red balls from left and right, then the state stays the same.

The first case happens with probability $P(X_n = i-1 | X_{n-1} = i) = \frac{i}{k} \cdot \frac{(k-i)}{k} = \frac{i(k-1)}{k^2}$. The second case happens by symmetry with the same probability. The third case happens with probability $P(X_n = i | X_{n-1} = i) = \frac{i^2}{k^2} + \frac{(k-i)^2}{k^2} = \frac{i^2+k^2 - 2ik + i^2}{k^2} = 1-\frac{2i(k-i)}{k^2}$. At the boundaries, $P(X_n = 1 | X_{n-1} =0) = 1$ and $P(X_n=k-1 | X_{n-1} = k) = 1$