---
title: "Chapter 1"
output: pdf_document
date: '2025-09-28'
---
\newcommand{\Var}{\textrm{Var}}
\newcommand{\Cov}{\textrm{Cov}}
\newcommand{\soln}{\textbf{Solution:}}
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Exercises
## Exercise 1.1

## Exercise 1.10
A die is rolled until a 3 occurs. By conditioning on the outcome of the first roll, find the probability that an even number of rolls is needed.

\soln Let $A$ be the event that the game is won. Let $X_i$ denote the $i$th roll. If $X_1$ is a 3, then the game fails. If $X_1$ is not a 3 but $X_2$ is a 3, then the game has won with probability 1. If $X_1$ nor $X_2$ are 3s, then the game goes back to the initial state, so the probability of winning is $p$. This recurrence relation is summarized below:
$$
P(A) = P(A \mid X_1 = 3)P(X_1 = 3) + P(A \mid X_1 \ne 3)P(X_1 \ne 3) \\
= P(A \mid X_1 = 3)P(X_1 = 3) + \left[P(A \mid X_1 \ne 3, X_2 \ne 3)P(X_2 \ne 3) \\+ P(A \mid X_1 \ne 3, X_2 = 3)P(X_2 = 3)\right]P(X_1 \ne 3) \\
p = 0\cdot \frac{1}{6} + \left(p\cdot \frac{5}{6} + 1 \cdot \frac{1}{6}\right)\frac{5}{6} \\
p = \frac{25}{36}p + \frac{5}{36} \\
\frac{11}{36}p = \frac{5}{36} \\
p = \frac{5}{11}
$$

## Exercise 1.11 NOT DONE
Consider the gambler's ruin process where at each wager, the gambler wins with probability $p$ and loses with probability $q = 1-p$. The gambler stops when reaching \$n or losing all their money. If the gambler starts with \$k, with $0<k<n$, find the probability of eventual ruin. See example 1.10.

\soln Let $p_k$ be the probability of winning the overall game with current wallet \$k, and $q_k$ be corresponding probability of losing. At current wallet \$k, the chance of winning restarts at the next step up or down, so we condition on the gambler winning or losing this round (LOTP).
$$
p_k = p \cdot p_{k+1} + q \cdot p_{k-1} \\
p\cdot p_k + q\cdot p_k = p\cdot p_{k+1} + q\cdot p_{k-1} \\
p(p_{k+1} - p_k) = q(p_k - p_{k-1}) \\
p_{k+1} - p_k = \frac{q}{p} (p_k-p_{k-1})
$$
Telescoping and multiplying both sides by $p/q$:
$$
p_1 - p_0 = \frac{p}{q}(p_2 - p_1) = \left(\frac{p}{q}\right)^2 (p_3-p_2) = \dots = \left(\frac{p}{q}\right)^{n-1}(p_n - p_{n-1})
$$

At the lower extreme, $p_1-p_0 = p_1$ since $p_0=0$. $p_1 = \frac{p}{q}(p_2-p_1)$, so $\frac{q}{p} p_1 + p_1 = p_2$ or $p_2 = \frac{1}{p}p_1$. Then, $\left(\frac{p}{q}\right)^2(p_3 - p_2) = \left(\frac{p}{q}\right)^2(p_3 - \frac{1}{p} p_1) = p_1$, so $\frac{p^2}{q^2}p_3 -\frac{p}{q^2}p_1 = p_1 $ so $p_3 = \frac{p+q^2}{p^2}p_1$


## Exercise 1.12
In $n$ rolls of a fair die, let $X$ be the number of times 1 is rolled, and $Y$ the number of times 2 is rolled. Find the conditional distribution of $X$ given $Y=y$.

\soln Since all the 2s have been counted, $X \mid Y$ can take on values $\{1,3,4,5,6\}$ with equal probability. There are $n-y$ remaining rolls, and the probability of getting a 1 is 1/5, so $X \mid Y = y \sim \textrm{Binom}(n-y,1/5)$. Note that the support for $X \mid Y=y$ are the integers $0 \le y \le n$, as $X \mid Y = y$ is Binomial.

## Exercise 1.13
Random variables $X$ and $Y$ have joint density function
$$
f(x,y) = 3y, \quad \textrm{ for }0 < x<y<1
$$
(a) Find the conditional density of $Y$ given $X=x$.

\soln The marginal density:

$$
f(x) = \int_Y f(x,y) dy = \int_x^1 3y dy = \left.\frac{3y^2}{2}\right|_x^1 = \frac{3-3x^2}{2}
$$

The conditional density:
$$
f(y\mid x) = \frac{f(x,y)}{f(x)} = \frac{3y}{(3-3x^2)/2} = \frac{2y}{1-x^2} \textrm{ for }0<x < y < 1
$$
(b) Find the conditional density of $Y$ given $X =x$. Describe the conditional distribution.

\soln The marginal distribution:
$$
f(y) = \int_X f(x,y) dx = \int_0^y 3ydx = \left.3yx\right|_0^y = 3y^2
$$
The conditional distribution:
$$
f(x \mid y) = \frac{f(x,y)}{f(y)} = \frac{3y}{3y^2} = \frac{1}{y}\textrm{ for }x < y < 1
$$

Note that this conditional distribution is a function of $y$, and does not depend on $x$ except for the bounds. Once $Y$ has been fixed, then $X$ can take on any value with uniform probability across the region $0 < x < y$.

## Exercise 1.14
Random variables $X$ and $Y$ have joint density function
$$
f(x,y) = 4e^{-2x}, \quad \textrm{for }0<y<x<\infty
$$
(a) Find the conditional density of $X$ given $Y=y$.

\soln The marginal density $f_Y(y)$ is:
$$
f_Y(y) = \int_X f(x,y) dx = \int_y^\infty 4e^{-2x} dx = \left.-2e^{-2x}\right|_{y}^\infty = 2e^{-2y}
$$

The conditional density is:
$$
f_{X\mid Y=y}(x\mid y) = \frac{f(x,y)}{f(y)} = \frac{4e^{-2x}}{2e^{-2y}} = 2e^{-2(x-y)}
$$
(b) Find the conditional density of $Y$ given $X=x$. Describe the conditional distribution.

\soln The marginal density:
$$
f_{X}(x) = \int_Y f(x,y) dy = \int_0^x 4e^{-2x} dy = \left.4ye^{-2x}\right|_0^x = 4xe^{-2x}
$$

The conditional density:
$$
f_{Y \mid X=x}(y \mid x) = \frac{f(x,y)}{f(x)} = \frac{4e^{-2x}}{4xe^{-2x}} = \frac{1}{x}
$$

This is a uniform distribution. Given that $X$ is a certain value, $y$ can take any value between 0 and $x$, so this is a uniform distribution on the interval (0, x) which is captured in the original limits of the joint density function.

## Exercise 1.15
Let $X$ and $Y$ be uniformly distributed on the disk of radius 1 centered at the origin. Find the conditional distribution of $Y$ given $X=x$.

\soln The joint distribution is:
$$
f_{X,Y}(x,y) = \frac{1}{\pi} \textrm{ for }x^2+ y^2 \le 1
$$
From the definition of conditional probability:
$$
f_{Y\mid X = x} (y\mid x) = \frac{f_{X,Y}(x,y)}{f_X(x)}
$$

The marginal distribution $f_X(x)$ can be found by integrating out the $Y$ variable from the joint distribution
$$
f_X(x) = \int_{Y} f_{X,Y}(x,y)dy = \int_{-\sqrt{1-x^2}}^{\sqrt{1-x^2}} \frac{1}{\pi} dy = \frac{2\sqrt{1-x^2}}{\pi}
$$

The conditional distribution is therefore:
$$
f_{Y\mid X=x}(y \mid x) = \frac{2\sqrt{1-x^2}/\pi}{1/\pi} = 2\sqrt{1-x^2} \textrm{ for } -1 \le x \le 1
$$

## Exercise 1.16
A poker hand consists of five cards drawn from a standard 52-card deck. Find the expected number of aces in a poker hand given that the first card drawn is an ace.

\soln Let $X$ be the total number of aces drawn and $A$ be the event that the first card is an ace. If $A$, then there are 3 aces to be drawn in the remaining 4 cards, so $X\mid A \sim \textrm{HGeom}(w=3,b=48,n=4)$, with an additional ace at the beginning. Therefore, 
$$
E(X \mid A) = 1 + 4\cdot\frac{3}{52} \approx 1.23
$$


## Exercise 1.17
Let $X$ be a Poisson random variable with $\lambda = 3$. Find $E(X\mid X > 2)$.

\soln The expression is:
$$
E(X\mid X>2) = \frac{\sum_{x=3}^\infty x\cdot P(X = x)}{P(X>2)}
$$

The numerator is:
$$
\begin{aligned}
\sum_{x=3}^\infty x\cdot P(X = x)
&= \underbrace{\sum_{x=0}^\infty x\cdot P(X=x)}_{=E(X)}- 0\cdot P(X=0) - 1\cdot P(X=1)-2\cdot P(X=2) \\
&= 3 - P(X=1) - 2P(X=2) \\
&= 3 - \frac{3^1e^{-3}}{1!} - 2\cdot \frac{3^2 e^{-3}}{2!} \\
&= 3 - e^{-3}(3+9) \\
&= 3-12e^{-3}
\end{aligned}
$$

The denominator is:
$$
\begin{aligned}
P(X>2) &= 1-P(X\le2) \\
&= 1-P(X=0) - P(X=1) - P(X=2) \\
&= 1-\frac{3^0 e^{-3}}{0!}-\frac{3^1 e^{-3}}{1!}-\frac{3^2 e^{-3}}{2!} \\
&= 1-e^{-3}(1+3+9/2) \\
&= 1-e^{-3}(17/2)
\end{aligned}
$$
Combining both, we have:
$$
E(X \mid X >2 ) = \frac{3-12e^{-3}}{1-8.5e^{-3}}
$$

## Exercise 1.18

From the definition of conditional expectation given an event, show that
$$
E(I_B\mid A) = P(B \mid A)
$$

\soln

$$
E(I_B \mid A) = \frac{E(I_B I_A)}{P(A)} = \frac{E(I_{A \cap B})}{P(A)} = \frac{P(A \cap B)}{P(A)} = P(B \mid A)
$$

## Exercise 1.19 NOT DONE

## Exercise 1.20
A fair coin is flipped repeatedly.
(a) Find the expected number of flips needed to get three heads in a row.

\soln These events partition the sample space: $T, HT, HHT,$ and $HHH$. Let $X$ be the flips to get 3 in a row.
$$
\begin{gathered}
E(X) = E(X \mid T)P(T) + E(X \mid HT)P(HT) + E(X \mid HHT)P(HHT) + E(X \mid HHH)P(HHH) \\
= E(X\mid T)\frac{1}{2} + E(X \mid HT)\frac{1}{4} + E(X \mid HHT)\frac{1}{8} + E(X \mid HHH)\frac{1}{8}
\end{gathered}
$$
If we get a tails, we restart. For simplicity, let $a = E(X)$. So $E(X\mid T) = 1 + E(X)$, $E(X\mid HT) = 2 + E(X)$, $E(X\mid HHT) = 3+E(X)$, and $E(X\mid HHH) = 3$:
$$
\begin{gathered}
a = \frac{1+a}{2} + \frac{2+a}{4} + \frac{3+a}{8} + \frac{3}{8} \\
a = \frac{1}{2} + \frac{2}{4} + \frac{3}{8} + \frac{3}{8} + a\left(\frac{1}{2} + \frac{1}{4} + \frac{1}{8}\right) \\
\frac{a}{8} = \frac{7}{4} \\
a = 14
\end{gathered}
$$

## Exercise 1.21
Let $T$ be a nonnegative, continuous random variable. Show
$$
\int_0^\infty P(T > t) dt = \int_0^\infty \left(\int_t^\infty f_T(x) dx \right) dt
$$
The inner integral runs from $t$ to $\infty$, and the outer integral from $0$ to $\infty$. Swapping the order of integration, the inner integral now runs from $t$ to $\infty$, while the outer integral runs from $0$ to $x$ (note that the bounds of integration coincide at the line $x=t$ if we imagine $x$ axis on the horizontal axis and $t$ on the vertical axis).

$$
\int_0^\infty \left(\int_t^\infty f_T(x) dx \right) dt = \int_0^\infty \left(\int_0^x f_T(x) dt \right) dx = \int_0^\infty f_T(x) \left(\int_0^x dt\right) dx = \int_0^\infty f_T(x) x dx = \int_0^\infty xf_T(x) dx = E(T)
$$

## Exercise 1.22
Let $E(Y\mid X)$ when $(X,Y)$ is uniformly distributed on the following regions.
(a) The rectangle $[a,b] \times [c,d]$.

\soln Since this is a rectangle and $(X,Y)$ is uniformly distributed, there is an equal likelihood of $Y$ appearing between $[c,d]$ regardless of where $X$ is, so $E(Y) = E(Y\mid X) = \frac{c+d}{2}$.

(b) The triangle with vertices (0,0),(1,0),(1,1).

\soln The joint distribution is:
$$
f_{X,Y}(x,y) = 2 \textrm{ for }0<y<x<1
$$

The marginal distribution of $X$ is:
$$
f_X(x) = \int_{Y} 2 dy = \int_{0}^x 2 dy = 2x
$$

The conditional distribution of $Y \mid X$ is:
$$
\begin{gathered}
f_{Y\mid X}(y \mid x) = \frac{f_{X,Y}(x,y)}{f_X(x)} = \frac{2}{2x} = \frac{1}{x} \\
E(Y \mid X) = \int_Y y \cdot f_{Y\mid X}(y \mid x) dy = \int_0^x y \cdot\frac{1}{x} dy = \int_0^x \frac{y}{x} dy = \left.\frac{y^2}{2x} \right|_0^x = \frac{x^2}{2x} = \frac{x}{2}
\end{gathered}
$$
(c) The disc of radius 1 centered at the origin.

\soln The joint distribution is:
$$
f_{X,Y}(x,y) = \frac{1}{\pi} \textrm{ for } x^2 + y^2 \le 1
$$

The marginal distribution of $X$:
$$
\begin{gathered}
f_X(x) = \int_Y f_{X,Y}(x,y) dy
= \int_{-\sqrt{1-x^2}}^{\sqrt{1-x^2}} \frac{1}{\pi} dy \\
= \left.\frac{y}{\pi}\right|_{-\sqrt{1-x^2}}^{\sqrt{1-x^2}} \\
= \frac{2\sqrt{1-x^2}}{\pi}
\end{gathered}
$$

Then the conditional distribution of $Y\mid X$ is:
$$
f_{Y\mid X}(y \mid x) = \frac{f_{X,Y}(x,y)}{f_{X}(x)} = \frac{1/\pi}{2\sqrt{1-x^2}/\pi} = \frac{1}{2\sqrt{1-x^2}}
$$

The conditional expectation $E(Y \mid X)$:
$$
E(Y \mid X) = \int_{Y} y \cdot f_{Y \mid X}(y\mid x) dy = \int_{-\sqrt{1-x^2}}^{\sqrt{1-x^2}} \frac{y}{2\sqrt{1-x^2}} dy = \left.\frac{y^2}{2\sqrt{1-x^2}}\right|_{-\sqrt{1-x^2}}^{\sqrt{1-x^2}} = \frac{1-x^2 - (1 - x^2)}{2\sqrt{1-x^2}} = 0
$$

## Exercise 1.23
Let $X_1, X_2, \dots$ be an i.i.d sequence random variables with common mean $\mu$. Let $S_n = X_1 + \cdots + X_n$ for $n \ge 1$.

(a) Find $E(S_M \mid S_n)$, for $m \le n$.

\soln Since $S_m = X_1 + \cdots + X_m$ and $S_n = S_m + X_{m+1} + \cdots + X_n$:

$$
\begin{gathered}
E(S_m \mid S_n) = E(S_n - X_{n} - \cdots - X_{m+1} \mid S_n) \\
= E(S_n \mid S_n) - E(X_n \mid S_n) - \cdots - E(X_{m+1} \mid S_n) \\
= S_n - E(X_n \mid S_n) - \cdots - E(X_{m+1} \mid S_n)
\end{gathered}
$$

By symmetry, $E(X_i \mid S_n) = S_n/n$, so $E(S_m\mid S_n) = S_n - S_n(n-m)/n = S_n(m/n)$.

(b) Find $E(S_m \mid S_n)$, for $m ge n$.

\soln 

$$
E(S_m \mid S_n) = E(S_n + X_{n+1}+\cdots+X_m \mid S_n) \\
= S_n + E(X_{m+1} \mid S_n) + \cdots + E(X_m \mid S_n)
$$

Since each $X_i$ is independent of $S_n$ for $i > n$, then $E(S_m \mid S_n) = S_n + (m-n)\mu$.

## Exercise 1.24

Prove the law of total expectation $E(Y) = E(E(Y\mid X))$ for the continuous case.

\soln
$$
E(E(Y\mid X)) = \int_{x} E(Y \mid X = x) f_{X}(x) dx \\
= \int_x \left( \int_y y f_{Y\mid X}(y\mid x) dy \right) f_{X}(x) dx
$$

The conditional distribution $f_{Y\mid X} = \frac{f_{x,y}(x, y)}{f_X{x}}$.

$$
\begin{gathered}
\int_x \left( \int_y y f_{Y\mid X}(y\mid x) dy \right) f_{X}(x) dx \\
= \int_x \left( \int_y y \frac{f_{X,Y}(x,y)}{f_X(x)} dy \right) f_{X}(x) dx \\
= \int_x \left( \int_y y f_{X}(x) \frac{f_{X,Y}(x,y)}{f_X(x)} dy \right)  dx \\
= \int_x  \int_y y f_{X,Y}(x,y) dy  dx \\
= \int_y y \left(\int_x f_{X,Y}(x,y) dx\right) dy \\ 
\int_y y f_Y(y) dy \\
= E(Y)
\end{gathered}
$$

## Exercise 1.25

Let $X$ and $Y$ be independent exponential random variables with respective parameters 1 and 2. Find $P(X/Y < 3)$ by conditioning.

\soln 

$$
\begin{aligned}
P(X/Y < 3) &= \int_{0}^\infty P(X/Y < 3 \mid Y = y) f_Y(y) dy \\
&=\int_{0}^\infty P(X/y < 3 \mid Y = y) f_Y(y) dy \\
&= \int_0^\infty P(X < 3y) P(Y = y) dy \\
&= \int_0^\infty (1-e^{-3y}) 2 e^{-2y} dy \\
&= \int_0^\infty 2e^{-2y} - 2e^{-5y} dy \\
&= \left.-e^{-2y}\right|_{0}^\infty + \frac{2}{5}\left.e^{-5y}\right|_{0}^\infty \\
&=1 -\frac{2}{5} \\
&=\frac{3}{5}
\end{aligned}
$$

## Exercise 1.26

The density of $X$ is $f(x) = xe^{-x}$, for $x > 0$. Given $X = x$, $Y$ is uniformly distributed on (0,x). Find $P(Y < 2)$ by conditioning on $X$.

\soln

We have $f_{Y\mid X}(y\mid x) = 1/x$, and $F_{Y\mid X}(y \mid x) = \frac{y}{x}$ for $0 < y < x$, and 1 for $y \ge x$.

If $X < 2$, then Y will always be between 0 and 2, so $P(Y < 2 \mid X < 2) = 1$. If $X \ge 2$, then $P(Y < 2 \mid X < 2) = 2/x$.

$$
\begin{aligned}
P(Y < 2) &= \int_0^\infty P(Y < 2 \mid X = x) f_X(x) dx \\
&= \int_0^\infty P(Y < 2 \mid X = x) xe^{-x} dx \\
&= \underbrace{\int_0^2 1 \cdot xe^{-x} dx}_{P(0 < X < 2)} + \int_2^\infty y/x\cdot xe^{-x} dx \\
&=  + \int_2^\infty 2 e^{-x} dx \\
&= (1- e^{-2}) - 2 e^{-x}\mid_{2}^\infty dx \\
&= 1 - e^{-2} + 2e^{-2}
\end{aligned}
$$

```{r}
exercise_1_26 <- function() {
  x <- rexp(1000, 1)
  y <- runif(1000, rep(0, 1000), x)
  mean(y < 2)
}
```

## Exercise 1.27

A restaurant receives $N$ customers per day, where $N$ is a random variable with mean 200 and standard deviation 40. The amount spent by each customer is normally distributed with mean \$15 and standard deviation \$3. The amounts that customers spend are independent of each other and independent of $N$. Find the mean and standard deviation of the total amount spent at the restaurant per day.

\soln Let $X_i$ be the amount that the $i$ customer spends, and $S = X_1 + \dots + X_N$.

$$
E(S) = E(E(S \mid N)) = E(E(X_1 + \dots + X_N \mid N)) = E(N X_i)= E(N)E(X_i) = 200 \cdot 15 = 3000
$$
$$
\begin{aligned}
Var(S) &= E(Var(S \mid N)) + Var(E(S \mid N)) \\
&= E(Var(X_1 + \dots + X_N \mid N)) + Var(E(X_1 + \dots + X_N \mid N)) \\
&= E(Var(X_1 \mid N) + \dots + Var(X_N \mid N)) + Var(E(X_1 \mid N) + \dots + E(X_N \mid N)) \\
&= E(N\cdot 3^2) + Var(N\cdot 15) \\
&= 200 \cdot 3^2 + 15^2\cdot 1600
\end{aligned}
$$

So the standard deviation is $\sqrt{361,800} = 30 \sqrt{201} \approx 601.50$.

```{r}
nsim <- 1000
Ns <- rnbinom(nsim, size = 25/(1-0.125), prob = 0.125)
Ss <- sapply(Ns, function(n) sum(rnorm(n, mean = 15, sd = 3)))
mean(Ss) # should be 3000
sd(Ss) # should be around 601.5
```
## Exercise 1.28
On any day, the number of accidents on the highway has a Poisson distributions with parameter $\Lambda$. The parameter $\Lambda$ varies from day to day and is itself a random variable. Find the mean and variance of the number of accidents per day when $\Lambda$ is uniformly distributed on (0,3).

\soln Let the total number of accidents be $T$, thus $T \mid \Lambda \sim \textrm{Pois}(\Lambda)$.

$$
\begin{aligned}
E(T) &= E(E(T\mid \Lambda)) = E(\Lambda) = \frac{3}{2} \\
Var(T) &= E(Var(T \mid \Lambda)) + Var(E(T \mid \Lambda)) \\
&= E(\Lambda) + Var(\Lambda) \\
&= \frac{3}{2} + \frac{3^2}{12} = \frac{27}{12} \\
&= \frac{9}{4}
\end{aligned}
$$

## Exercise 1.29

If $X$ and $Y$ are independent, does $Var(Y\mid X) = Var(Y)$?

\soln Yes, since $Var(Y\mid X) = E(Y^2 \mid X) - E(Y \mid X)^2 = E(Y^2) - E(Y)^2 = Var(Y)$.

## Exercise 1.30
Assume that $Y = g(X)$ is a function of $X$. Find simple expressions for
(a) $E(Y\mid X)$.

\soln $E(g(X) \mid X) = g(X)$

(b) $Var(Y \mid X)$.

\soln $Var(Y \mid X) = E(Y^2 \mid X) - E(Y\mid X)^2 = E(g(X)^2 \mid X) - E(g(X) \mid X)^2 = g(X)^2 - g(X)^2 = 0$.

## Exercise 1.31 NOT DONE
Consider a sequence of i.i.d. Bernoulli trials with success parameter $p$. Let $X$ be the number of tirals needed until the first success occurs. Then, $X$ has a geometric distribution with parameter $p$. Find the variance of $X$ by conditioning on the first trial.

\soln Let $T_1$ be the outcome of the first trial. If the first trial succeeds, then $X = 1$. Otherwise, if the trial fails, then we restart the game. The cases are:
$$
X = \begin{cases}
0 &\quad \text{with probability }p \\
1+X &\quad \text{with probability }1-p
\end{cases}
$$

$$
Var(X) = E(Var(X \mid T_1)) + Var(E(X \mid T_1)) \\
= E()
$$

## Exercise 1.32

R: Simulate flipping three fair coins and counting the number of heads $X$.

(a) Use your simulation to estimate $P(X=1)$ and $E(X)$.

\soln

```{r exercise 1.32.a}
nsim <- 10000
heads_from_fair_coin <- rbinom(nsim, 3, 0.5)
mean(heads_from_fair_coin == 1) # P(X=1)
mean(heads_from_fair_coin) # E(X)
```

(b) Modify the above to allow for a biased coin where $P$(Heads) = 3/4.

\soln

```{r exercise 1.32.b}
nsim <- 10000
p_biased_coin <- 0.75
heads_from_biased_coin <- rbinom(nsim, 3, p_biased_coin)
mean(heads_from_biased_coin == 1) # P(X=1)
mean(heads_from_biased_coin) # E(X)
```

## Exercise 1.33

R: Cards are drawn from a standard deck, with replacement, until an ace appears. Simulate the mean and variance of the number of cards required.

\soln 


